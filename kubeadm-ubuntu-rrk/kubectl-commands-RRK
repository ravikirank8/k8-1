Author : Ravi Ravada

=============================================================================================================================================

explain manifest file of k8s (AKSM)

POD:

3)Pod encapsulation  an application container, storage , network.
4)By default POD use untlimited resources(CPU and memory)
1)Kubernetes  manages the pods rather container directly
2)Pods represent a running process in your cluster.


 kubectl run custompod --image=vadisala/rvadisala_docker_repo:nginx-custom --restart=Never --dry-run=client -o yaml

 kubectl run vad --image=vadisala/rvadisala_docker_repo:nginx-custom  --env=team=noc --dry-run=client -o yaml > b.yml

kubectl run busybox --image=busybox --restart=Never -o yaml --dry-run=client  -- /bin/sh -c   'while true;do echo amobee;sleep  5;done' > pod1.yaml

 kubectl logs  -f busybox -c busybox

kubectl exec busybox -c busybox -- ls

 kubectl run rvada --image=vadisala/rvadisala_docker_repo:nginx-custom
 
 

Mutlicontainer
=============

Containers in a Pod run on a “logical host”; they use the same network namespace (in other words, the same IP address and port space),
and the same IPC namespace. They can also use shared volumes. 
These properties make it possible for these containers to efficiently communicate, ensuring data locality. 

The 1st container runs nginx server and has the shared volume mounted to the directory /usr/share/nginx/html.
The 2nd container uses the Debian image and has the shared volume mounted to the directory /html.
Every second, the 2nd container adds the current date and time into the index.html file, which is located in the shared volume.
When the user makes an HTTP request to the Pod, the Nginx server reads this file and transfers it back to the user in response to the request.


========

 This is important because with multiple processes in the same container,
 it is harder to troubleshoot the container because logs from different processes will be mixed together,
 and it is harder to manage the processes lifecycle, for example to take care of “zombie” processes when their parent process dies.
 Second, using several containers for an application is simpler and more transparent, and enables decoupling software dependencies. 
 Also, more granular containers can be reused between teams.
 
 Sidecar containers “help” the main container. Some examples include log or data change watchers, monitoring adapters, and so on.



apiVersion: v1
kind: Pod
metadata:
  name: mc
spec:
  containers:
  - name: web
    image: nginx
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
  - name: os
    image: centos
    volumeMounts:
    - name: html
      mountPath: /html
    command: ["/bin/sh", "-c"]
    args:
      - while true; do
          date >> /html/index.html;
          sleep 1;
        done
  volumes:
  - name: html
    emptyDir: {}
============================

apiVersion: v1
kind: Pod
metadata:
  labels:
    run: mcs
  name: mcs
spec:
  containers:
  - image: nginx
    name: web
    volumeMounts:
    - name: html
      mountPath: /html
  - image: centos:latest
    name: os1
    volumeMounts:
    - name: html
      mountPath: /test1
    command: ['sh', '-c', 'sleep 3600']
  - image: ubuntu
    name: os2
    volumeMounts:
    - name: html
      mountPath: /test2
    command: ['sh', '-c', 'sleep 3600']
  - image: centos:7
    name: os3
    volumeMounts:
    - name: html
      mountPath: /test3
    command: ['sh', '-c', 'sleep 3600']
  - image: redis
    name: os4
    volumeMounts:
    - name: html
      mountPath: /html2
 volumes:
 - name: html
   emptyDir: {}
=================
kubectl exec -it mc1 -c web -- /bin/sh
kubectl exec -it mc1 -c web -- /bin/sh

==================================================================

=====================
Namespaces
============

Namespaces are Kubernetes objects which partition a single Kubernetes cluster into multiple virtual clusters.

By default, a Kubernetes cluster is created with the following three namespaces:

Kubernetes objects which partition a single Kubernetes cluster into multiple virtual clusters.

default: By default all the resource created in Kubernetes cluster are created in the default namespace. 
By default the default namespace can allow applications to run with unbounded CPU and memory requests/limits 
(Until someone set resource quota for the default namespace).
kube-public: Namespace for resources that are publicly readable by all users. This namespace is generally reserved for cluster usage.
kube-system: It is the Namespace for objects created by Kubernetes systems/control plane.
kube-node-lease This namespace holds Lease objects associated with each node.
Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure. 
 
1)Namespaces is like partitioning the k8 cluster 

2)Like we have difference environments like dev, prod.

3)They can be helpful when different teams or projects share a Kubernetes cluster

List all K8s API supported Objects and Versions
kubectl api-resources
kubectl api-versions

Man pages for objects
kubectl explain <object>.<option>
kubectl explain pod
kubectl explain pod.apiVersion
kubectl explain pod.spec


kubectl create namespace mynamespace

kubectl config view | grep namespaces

 kubectl api-resources  --namespaced=false
 
#TO SET CURRENT NAMESPACE
kubectl config set-context --current --namespace=prod

kubectl create namespace mynamespace

kubectl run nginx --image=nginx --restart=Never -n mynamespace

 kubectl run nginx2 --image=nginx --port=80  --dry-run=client -o yaml > namespae.yml

kubectl get po --all-namespaces
kubectl run nginx --image=nginx --restart=Never --port=80

 ============================================================================================================
 Lables and Selectors:
 ======================
 
 Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects 
 
 kubectl run fronend --image=nginx --restart=Never --labels=env=prod,team=amobee --dry-run=client -o yaml

 kubectl run fronend --image=nginx --restart=Never --labels=env=dev,team=turn,app=2.0 --dry-run=client -o yaml

  kubectl run fronend --image=nginx --restart=Never --labels=env=prod,team=noc
 
  kubectl run fronend1 --image=nginx --restart=Never --labels=env=prod,team=techops,app=v2.0
 
  kubectl run fronend3 --image=nginx --restart=Never --labels=env=prod,team=techops,app=amobee
 
 kubectl run fronend4 --image=nginx --restart=Never --labels=env=prod,team=turn
 
 kubectl get pods -l 'team in(noc,techops)'   --show-labels
    
  kubectl get po --show-labels  
   
  kubectl get pods -l 'team in(noc,techops)',env=prod  --show-labels
  
 Annotations
 ============
 Annotations allow you to add non-identifying metadata to Kubernetes objects.
 Examples include phone numbers of persons responsible for the object or tool information for debugging purposes.
 In short, annotations can hold any kind of information that is useful and can provide context to DevOps teams

 kubectl annotate pod/fronend contact="noc"
 
 Environment:
 
. When you create a Pod, you can set environment variables for the containers that run in the Pod.
. To set environment variables, include the env  field in the configuration file.

    

=====================================================================================

* Replication Controller and Replica Set 

Replica Set is the next generation of Replication Controller. Replication controller is kinda imperative, but replica sets try to be as declarative as possible.

1.The main difference between a Replica Set and a Replication Controller right now is the selector support.

+--------------------------------------------------+-----------------------------------------------------+
|                   Replica Set                    |               Replication Controller                |
+--------------------------------------------------+-----------------------------------------------------+
| Replica Set supports the new set-based selector. | Replication Controller only supports equality-based |
| This gives more flexibility. for eg:             | selector. for eg:                                   |
|          environment in (production, qa)         |             environment = production                |
|  This selects all resources with key equal to    | This selects all resources with key equal to        |
|  environment and value equal to production or qa | environment and value equal to production           |
+--------------------------------------------------+-----------------------------------------------------+
2.The second thing is the updating the pods.
+-------------------------------------------------------+-----------------------------------------------+
|                      Replica Set                      |            Replication Controller             |
+-------------------------------------------------------+-----------------------------------------------+
| rollout command is used for updating the replica set. | rolling-update command is used for updating   |
| Even though replica set can be used independently,    | the replication controller. This replaces the |
| it is best used along with deployments which          | specified replication controller with a new   |
| makes them declarative.                               | replication controller by updating one pod    |
|                                                       | at a time to use the new PodTemplate.         |
+-------------------------------------------------------+---------------------------------

here is also a small difference in the syntax between Replica Controller:

apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    app: nginx
    
And the ReplicaSet which contains matchLabels field under the selector:

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels: #<-- This was added
      tier: nginx










 
Deployment
==============

kubectl create deployment my-dep --image=nginx --replicas=3 --dry-run=client -o yaml
 
kubectl scale deploy my-dep  --replicas=5

kubectl get po

kubectl describe deploy nginx

kubectl create deploy nginx --image=nginx:1.18.0 --replicas=2 --port=80

kubectl create deploy nginx --image=nginx:1.18.0 --replicas=2 --port=80 --dry-run=client  -o yaml > a.yml

kubectl scale --replicas=7 rs nginx

kubectl edit rs nginx

kubectl get rc
kubectl get deploy
kubectl get deploy nginx -o yaml
============================================================================================================
Rollback:
=========

Kubernetes Rolling:
Check how the deployment rollout is going
kubectl rollout status deploy nginx

kubectl set image deploy nginx nginx=nginx:1.19.8

kubectl edit deploy nginx

Check the rollout history and confirm that the replicas are OK

kubectl rollout history deploy nginx
kubectl rollout undo deploy nginx

kubectl get deploy nginx
kubectl get rs  # check that a new replica set has been created
kubectl get po
kubectl scale --replicas=7 rs nginx

==============================================================================================================

Scheduler
=======
 
kubectl run busybox --image=busybox  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod1.yaml

Multicontainers

// create the pod

kubectl create -f simple-pod.yml// access logs

kubectl logs busybox -c busybox1
kubectl logs busybox -c busybox2
kubectl logs busybox -c busybox3// exec into running containers


Mutlicontainer in POD
========================

kubectl run busybox --image=busybox  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml

kubectl exec -it busybox -- /bin/bas

images u can use for minimal container
=========================================

alpine
busybox
centos:latest 
ubuntu:20.04

kubectl exec -it busybox -c busybox1 /bin/sh
kubectl exec -it busybox -c busybox2 /bin/sh
kubectl exec -it busybox -c busybox3 /bin/sh



 kubectl run busybox --image=busybox  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml


kubectl exec -it busybox -- /bin/bash


================================================
Environment:

Of course, we have solved this problem. You can use environment variables and configuration files to store this information in a “central location” and reference 
these from our app. When you want to change the configuration, simply change it in the file or change the environment variable, 
and you are good to go! No need to hunt down every location where the data is referenced.



 kubectl run busybox --image=busybox  --env=team=noc  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml

 kubectl exec busybox -- printenv

 kubectl exec -it busybox -- env

 kubectl exec -it busybox -- /bin/sh


==========================================

Config maps

kubectl create cm mycm --from-file=a.txt
kubectl get cm
kubectl describe cm mycm

kubectl create cm mycm --from-file=a.txt --dry-run=client -o yaml

We see this using by creating it using test pod

kubectl create cm variables --from-file=variables

 kubectl run busybox --image=busybox  --env=team=noc  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml

kubectl exec -it busybox -- env

kubectl get cm -o yaml

    
kubectl create secret generic sec --from-file=ssh-private=/root/.ssh/known_hosts --from-literal=passphrase=ravikiran

 kubectl create secret generic db-user-pass  --from-file=a.txt  --from-file=pass.txt --dry-run=client -o yaml

kubectl create secret generic sec --from-file=usercred=ravikiran --from-literal=passphrase=ravikiran300 --dry-run=client -o yaml

=====================================================================================================================================


Node node01 Unschedulable
Pods evicted from node01 

#We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable
kubectl drain node01 --ignore-daemonsets

Node01 is Schedulable
kubectl uncordon node01

Run: kubectl get pods -o wide and you will see that there is a single pod scheduled on node01 which is not part of a replicaset.
The drain command will not work in this case. To forcefully drain the node we now have to use the --force flag.

We need to carry out a maintenance activity on node01 again. Try draining the node again using the same command as before: 

A forceful drain of the node will delete any pod that is not part of a replicaset

This means that both nodes have the ability to schedule workloads on them
 kubectl describe nodes controlplane | grep -i taints 
 
 To get latest version of k8 cluster
 kubectl upgrade plan
 
 We will be upgrading the master node first. Drain the master node of workloads and mark it UnSchedulable
 There are daemonsets created in this cluster, especially in the kube-system namespace. To ignore these objects and drain the node, we can make use of the --ignore-daemonsets flag
 kubectl drain controlplane --ignore-daemonsets
 
 
 In order to ensure minimum downtime, upgrade the cluster one node at a time, while moving the workloads to another node. In the upcoming tasks you will get to practice how to do that
 
 Upgrade the controlplane components to exact version v1.20.0

Upgrade kubeadm tool (if not already), then the master components, and finally the kubelet. Practice referring to the kubernetes documentation page. Note: While upgrading kubelet, if you hit dependency issue while running the apt-get upgrade kubelet command, use the apt install kubelet=1.20.0-00 command instead
 
 On the controlplane node, run the command run the following commands:

    apt update
    This will update the package lists from the software repository.

    apt install kubeadm=1.20.0-00
    This will install the kubeadm version 1.20

    kubeadm upgrade apply v1.20.0
    This will upgrade kubernetes controlplane. Note that this can take a few minutes.

    apt install kubelet=1.20.0-00 This will update the kubelet with the version 1.20.

    You may need to restart kubelet after it has been upgraded.
    Run: systemctl restart kubelet

============

Upgrade the controlplane components to exact version v1.20.0

Upgrade kubeadm tool (if not already), then the master components, and finally the kubelet. Practice referring to the kubernetes documentation page. Note: While upgrading kubelet, if you hit dependency issue while running the apt-get upgrade kubelet command, use the apt install kubelet=1.20.0-00 command instead

make it schudable again
kubectl uncordon controlplane

====================================
kubectl get po -n kube-system

Look at the ETCD Logs using the command kubectl logs etcd-controlplane -n kube-system or check the image used by the ETCD pod: 

kubectl describe pod etcd-controlplane -n kube-system

At what address can you reach the ETCD cluster from the controlplane node?

Check the ETCD Service configuration in the ETCD POD
kubectl -n kube-system describe pod etcd-controlplane | grep '\--listen-client-urls'

etc-cert file
kubectl -n kube-system describe pod etcd-controlplane | grep '\--cert-file'

root@controlplane:~# kubectl -n kube-system describe pod etcd-controlplane | grep '\--trusted-ca-file'

snapshot
===========

The master nodes in our cluster are planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.

Store the backup file at location /opt/snapshot-pre-boot.db

se the etcdctl snapshot save command. You will have to make use of additional flags to connect to the ETCD server.
--endpoints: Optional Flag, points to the address where ETCD is running (127.0.0.1:2379)
--cacert: Mandatory Flag (Absolute Path to the CA certificate file)
--cert: Mandatory Flag (Absolute Path to the Server certificate file)
--key:Mandatory Flag (Absolute Path to the Key file) 

ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db


Snapshot saved at /opt/snapshot-pre-boot.db


Restore the snapshot
====================

Restore the etcd to a new directory from the snapshot by using the etcdctl snapshot restore command. Once the directory is restored, update the ETCD configuration to use the restored directory.

root@controlplane:~# ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db


2021-03-25 23:52:59.608547 I | mvcc: restore compact to 6466
2021-03-25 23:52:59.621400 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32

Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the --data-dir.

Next, update the /etc/kubernetes/manifests/etcd.yaml:

We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory (/var/lib/etcd) to the new directory /var/lib/etcd-from-backup.

 volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
    
    With this change, /var/lib/etcd on the container points to /var/lib/etcd-from-backup on the controlplane (which is what we want)

When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the /etc/kubernetes/manifests directory.

    Note: as the ETCD pod has changed it will automatically restart, and also kube-controller-manager and kube-scheduler. Wait 1-2 to mins for this pods to restart. You can run a watch "docker ps | grep etcd" command to see when the ETCD pod is restarted.

    Note2: If the etcd pod is not getting Ready 1/1, then restart it by kubectl delete pod -n kube-system etcd-controlplane and wait 1 minute.

    Note3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.

If you do change --data-dir to /var/lib/etcd-from-backup in the YAML file, make sure that the volumeMounts for etcd-data is updated as well, with the mountPath pointing to /var/lib/etcd-from-backup (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)

===========================

Taints and Tolerations
=======================

Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule

    Key = spray
    Value = mortein 

kubectl taint nodes node01 spray=mortein:NoSchedule

kubectl describe node01 | grep -i taints

=======

Create another pod named bee with the NGINX image, which has a toleration set to the taint mortein.

    Image name: nginx
    Key: spray
    Value: mortein
    Effect: NoSchedule
    Status: Running
    
    ===
    
    Create another pod named bee with the NGINX image, which has a toleration set to the taint mortein.

    Image name: nginx
    Key: spray 
    
    ==============================
  
  Node Affinity
  ==============
  
Set Node Affinity to the deployment to place the pods on node01 only.

    Name: blue
    Replicas: 3
    Image: nginx
    NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
    Key: color
    values: blue 
    
    apiVersion: v1
kind: Pod
metadata:
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    value: mortein
    effect: NoSchedule
    operator: Equal
    
    remove taint:
    check where is taint in mastet or node
    
    kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-
    
    
    ========================
    
     kubectl label node node01 color=blue
     
     Check if controlplane and node01 have any taints on them that will prevent the pods to be scheduled on them. If there are no taints, the pods can be scheduled on either node.
So run the following command to check the taints on both nodes.
kubectl describe node controlplane | grep -i taints
kubectl describe node node01 | grep -i taints





Update the deployment by running kubectl edit deployment blue and add the nodeaffinity section as follows:

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue

==================================
Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.

Use the label - node-role.kubernetes.io/master - set on the controlplane node.

    Name: red
    Replicas: 2
    Image: nginx
    NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
    Key: node-role.kubernetes.io/master
    Use the right operator 



apiVersion: apps/v1
kind: Deployment
metadata:
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: Exists
                
                
                
 =========================================================================================================================================
 ==========================================================================================================================================
 
 Volumes
 ======
 volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}
    
    
 image: busybox
    name: busybox
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: DirectoryOrCreate
      
      
     =============================================
 
#https://betterprogramming.pub/tutorial-how-to-use-kubernetes-job-and-cronjob-1ef4ffbc8e84

look into this article about jobs and cron jobs
     
     Job:
     kubectl create job1 rav --image=busybox -o yaml  --dry-run=client   -- /bin/sh -c 'date; echo sleeping....; sleep 90s; echo exiting...'


Enforcing a time limit
For e.g., you are running a batch job and it takes too long to finish due to some reason. This might be undesirable. 
You can limit the time for which a Job can continue to run by setting the activeDeadlineSeconds attribute in the spec.

spec:
  activeDeadlineSeconds: 5
  template:

=====

Handling failures

What if there are issues due to container failure (process exited) or Pod failure? Let's try this out by simulating a failure.

In this Job, the container prints the date, sleeps for 5 seconds, and exits with a status 1 to simulate failure.

here set sleep 5

spec:
  backoffLimit: 2
  template:

==================

cronjob
========

kubectl create cronjob busybox --image=busybox --schedule="*/1 * * * *"   -o yaml --dry-run=client -- /bin/sh -c 'date; echo Hello from the Kubernetes cluster'


serives
======
#https://github.com/kodekloudhub/certified-kubernetes-administrator-course/blob/master/docs/02-Core-Concepts/19-Services.md
LoadBalacer
#https://github.com/dennyzhang/cheatsheet-kubernetes-A4

kubectl expose deployment/my-demo-website --type=LoadBalancer --dry-run=client -o yaml > svc.yaml
kubectl expose deployment/mydemo --type=LoadBalancer --dry-run=client -o yaml > svc.yaml

NodePort:
========
 kubectl expose deployment/mydemo --type=NodePort --dry-run=client -o yaml  >

ClusterIP:
========

https://matthewpalmer.net/kubernetes-app-developer/articles/service-kubernetes-example-tutorial.html

Cluster IP is a virtual IP that is allocated by the K8s to a service. It is K8s internal IP.

A Cluster IP makes it accessible from any of the Kubernetes cluster’s nodes. The use of virtual IP addresses for this purpose makes it possible to have several pods expose the same port on the same node – All of these pods will be accessible via a unique IP address.

This IP is stable and never changes in the service lifecycle(unless deleted explicitly).

2 different pods can communicate using this IP, though I recommend using cluster DNS service

kubectl expose deployment/mydemo --dry-run=client -o yaml

=========

autoscale
=========

kubectl autoscale deploy/mydemo --min=5 --max=10 --cpu-percent=80 --dry-run=client -o yaml

kubectl create quota myrq --hard=cpu=1,memory=1G,pods=2 --dry-run=client -o yaml



     
     

    





 
 
 
