Author : Ravi Ravada

=============================================================================================================================================

explain manifest file of k8s (AKSM)

POD:

3)Pod encapsulation  an application container, storage , network.
4)By default POD use untlimited resources(CPU and memory)
1)Kubernetes  manages the pods rather container directly
2)Pods represent a running process in your cluster.


 kubectl run custompod --image=vadisala/rvadisala_docker_repo:nginx-custom --restart=Never --dry-run=client -o yaml

 kubectl run vad --image=vadisala/rvadisala_docker_repo:nginx-custom  --env=team=noc --dry-run=client -o yaml > b.yml

kubectl run busybox --image=busybox --restart=Never -o yaml --dry-run=client  -- /bin/sh -c   'while true;do echo amobee;sleep  5;done' > pod1.yaml

 kubectl logs  -f busybox -c busybox

kubectl exec busybox -c busybox -- ls

 kubectl run rvada --image=vadisala/rvadisala_docker_repo:nginx-custom
 
 Flannel:
 does not support Network Policies

Calico:
supports Network Policies

Calico, Flannel, Weave and Cilium medium all support network polices

 
 

Mutlicontainer
=============

Containers in a Pod run on a “logical host”; they use the same network namespace (in other words, the same IP address and port space),
and the same IPC namespace. They can also use shared volumes. 
These properties make it possible for these containers to efficiently communicate, ensuring data locality. 

The 1st container runs nginx server and has the shared volume mounted to the directory /usr/share/nginx/html.
The 2nd container uses the Debian image and has the shared volume mounted to the directory /html.
Every second, the 2nd container adds the current date and time into the index.html file, which is located in the shared volume.
When the user makes an HTTP request to the Pod, the Nginx server reads this file and transfers it back to the user in response to the request.


========

 This is important because with multiple processes in the same container,
 it is harder to troubleshoot the container because logs from different processes will be mixed together,
 and it is harder to manage the processes lifecycle, for example to take care of “zombie” processes when their parent process dies.
 Second, using several containers for an application is simpler and more transparent, and enables decoupling software dependencies. 
 Also, more granular containers can be reused between teams.
 
 Sidecar containers “help” the main container. Some examples include log or data change watchers, monitoring adapters, and so on.



apiVersion: v1
kind: Pod
metadata:
  name: mc
spec:
  containers:
  - name: web
    image: nginx
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
  - name: os
    image: centos
    volumeMounts:
    - name: html
      mountPath: /html
    command: ["/bin/sh", "-c"]
    args:
      - while true; do
          date >> /html/index.html;
          sleep 1;
        done
  volumes:
  - name: html
    emptyDir: {}
============================

apiVersion: v1
kind: Pod
metadata:
  labels:
    run: mcs
  name: mcs
spec:
  containers:
  - image: nginx
    name: web
    volumeMounts:
    - name: html
      mountPath: /html
  - image: centos:latest
    name: os1
    volumeMounts:
    - name: html
      mountPath: /test1
    command: ['sh', '-c', 'sleep 3600']
  - image: ubuntu
    name: os2
    volumeMounts:
    - name: html
      mountPath: /test2
    command: ['sh', '-c', 'sleep 3600']
  - image: centos:7
    name: os3
    volumeMounts:
    - name: html
      mountPath: /test3
    command: ['sh', '-c', 'sleep 3600']
  - image: redis
    name: os4
    volumeMounts:
    - name: html
      mountPath: /html2
 volumes:
 - name: html
   emptyDir: {}
=================
kubectl exec -it mc1 -c web -- /bin/sh
kubectl exec -it mc1 -c web -- /bin/sh

==================================================================

=====================
Namespaces
============

Namespaces are Kubernetes objects which partition a single Kubernetes cluster into multiple virtual clusters.

By default, a Kubernetes cluster is created with the following three namespaces:

Kubernetes objects which partition a single Kubernetes cluster into multiple virtual clusters.

default: By default all the resource created in Kubernetes cluster are created in the default namespace. 
By default the default namespace can allow applications to run with unbounded CPU and memory requests/limits 
(Until someone set resource quota for the default namespace).
kube-public: Namespace for resources that are publicly readable by all users. This namespace is generally reserved for cluster usage.
kube-system: It is the Namespace for objects created by Kubernetes systems/control plane.
kube-node-lease This namespace holds Lease objects associated with each node.
Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure. 
 
1)Namespaces is like partitioning the k8 cluster 

2)Like we have difference environments like dev, prod.

3)They can be helpful when different teams or projects share a Kubernetes cluster

List all K8s API supported Objects and Versions
kubectl api-resources
kubectl api-versions

Man pages for objects
kubectl explain <object>.<option>
kubectl explain pod
kubectl explain pod.apiVersion
kubectl explain pod.spec


kubectl create namespace mynamespace

kubectl config view | grep namespaces

 kubectl api-resources  --namespaced=false
 
#TO SET CURRENT NAMESPACE
kubectl config set-context --current --namespace=prod

kubectl create namespace mynamespace

kubectl run nginx --image=nginx --restart=Never -n mynamespace

 kubectl run nginx2 --image=nginx --port=80  --dry-run=client -o yaml > namespae.yml

kubectl get po --all-namespaces
kubectl run nginx --image=nginx --restart=Never --port=80

 ============================================================================================================
 Lables and Selectors:
 ======================
 
 Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects 
 
 kubectl run fronend --image=nginx --restart=Never --labels=env=prod,team=amobee --dry-run=client -o yaml

 kubectl run fronend --image=nginx --restart=Never --labels=env=dev,team=turn,app=2.0 --dry-run=client -o yaml

  kubectl run fronend --image=nginx --restart=Never --labels=env=prod,team=noc
 
  kubectl run fronend1 --image=nginx --restart=Never --labels=env=prod,team=techops,app=v2.0
 
  kubectl run fronend3 --image=nginx --restart=Never --labels=env=prod,team=techops,app=amobee
 
 kubectl run fronend4 --image=nginx --restart=Never --labels=env=prod,team=turn
 
 kubectl get pods -l 'team in(noc,techops)'   --show-labels
    
  kubectl get po --show-labels  
   
  kubectl get pods -l 'team in(noc,techops)',env=prod  --show-labels
  
 Annotations
 ============
 Annotations allow you to add non-identifying metadata to Kubernetes objects.
 Examples include phone numbers of persons responsible for the object or tool information for debugging purposes.
 In short, annotations can hold any kind of information that is useful and can provide context to DevOps teams

 kubectl annotate pod/fronend contact="noc"
 
 Environment:
 
. When you create a Pod, you can set environment variables for the containers that run in the Pod.
. To set environment variables, include the env  field in the configuration file.

    

=====================================================================================

* Replication Controller and Replica Set 

A simple case is to create one ReplicationController object to reliably run one instance of a Pod indefinitely.
A more complex use case is to run several identical replicas of a replicated service, such as web servers

If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the ReplicationController starts more pods
=====================



Replica Set is the next generation of Replication Controller. Replication controller is kinda imperative, but replica sets try to be as declarative as possible.

1.The main difference between a Replica Set and a Replication Controller right now is the selector support.

+--------------------------------------------------+-----------------------------------------------------+
|                   Replica Set                    |               Replication Controller                |
+--------------------------------------------------+-----------------------------------------------------+
| Replica Set supports the new set-based selector. | Replication Controller only supports equality-based |
| This gives more flexibility. for eg:             | selector. for eg:                                   |
|          environment in (production, qa)         |             environment = production                |
|  This selects all resources with key equal to    | This selects all resources with key equal to        |
|  environment and value equal to production or qa | environment and value equal to production           |
+--------------------------------------------------+-----------------------------------------------------+
2.The second thing is the updating the pods.
+-------------------------------------------------------+-----------------------------------------------+
|                      Replica Set                      |            Replication Controller             |
+-------------------------------------------------------+-----------------------------------------------+
| rollout command is used for updating the replica set. | rolling-update command is used for updating   |
| Even though replica set can be used independently,    | the replication controller. This replaces the |
| it is best used along with deployments which          | specified replication controller with a new   |
| makes them declarative.                               | replication controller by updating one pod    |
|                                                       | at a time to use the new PodTemplate.         |
+-------------------------------------------------------+---------------------------------

here is also a small difference in the syntax between Replica Controller:

apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    app: nginx
    
And the ReplicaSet which contains matchLabels field under the selector:

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels: #<-- This was added
      tier: nginx
      
 ==============================================================
 
Deployment
A ReplicaSet ensures that a specified number of pod replicas are running at any given time.
However, Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features.
 
Deployment
==============

kubectl create deployment my-dep --image=nginx --replicas=3 --dry-run=client -o yaml
 
kubectl scale deploy my-dep  --replicas=5

kubectl get po

kubectl describe deploy nginx

kubectl create deploy nginx --image=nginx:1.18.0 --replicas=2 --port=80

kubectl create deploy nginx --image=nginx:1.18.0 --replicas=2 --port=80 --dry-run=client  -o yaml > a.yml

kubectl scale --replicas=7 rs nginx

kubectl edit rs nginx

kubectl get rc
kubectl get deploy
kubectl get deploy nginx -o yaml


apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

============================================================================================================
Rollback:
=========

https://github.com/kodekloudhub/certified-kubernetes-administrator-course/blob/master/docs/05-Application-Lifecycle-Management/02-RollingUpdates-and-Rollback.md

https://www.educative.io/blog/kubernetes-deployments-strategies

Kubernetes Rolling:
Check how the deployment rollout is going
kubectl rollout status deploy nginx

kubectl set image deploy nginx nginx=nginx:1.19.8

kubectl edit deploy nginx

Check the rollout history and confirm that the replicas are OK

kubectl rollout history deploy nginx
kubectl rollout undo deploy nginx

kubectl get deploy nginx
kubectl get rs  # check that a new replica set has been created
kubectl get po
kubectl scale --replicas=7 rs nginx

==============================================================================================================

Scheduler
=======

Manual Scheduling :  nodeName: node02
 
kubectl run busybox --image=busybox  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod1.yaml

====

Taints and Tolearations:

taints and toleration are like what pods to be schudled on nodes

taints are set on nodes and tolerations are set on pods

Taints and Tolerations are used to set restrictions on what pods can be scheduled on a node.
Only pods which are tolerant to the particular taint on a node will get scheduled on that node.


taint: is the command to apply taints in the nodes
nodes: are set of worker nodes
nodename: is the name of the specific worker node, on which taint has to be applied, it has a key-value pair
key-value pair: it is used to specify which application type in the pod will this node may be attached
taint-effect: This is used to define how the pod will be treated if they are not tolerant of the taint.

The effects are as below;

    NoSchedule — Pods will not be schedule on the nodes
    PreferNoSchedule — The system will try to avoid placing a pod on the node, but it’s not guaranteed
    NoExecute — New pods will not be scheduled on the node and existing pods on the node if any will be evicted if they do not tolerate the taint
    
    kubectl taint nodes node1 app=blue:NoSchedule
    
 Tolerations are added to pods by adding a tolerations section in pod definition.
 
 Node Selectors
 ================
 
 nodeSelector:
  size: Large
  
  $ kubectl label nodes node-1 size=Large
 
 Node Affinity:
 ===============
 
 The primary feature of Node Affinity is to ensure that the pods are hosted on particular nodes.

 With Node Selectors we cannot provide the advance expressions.
 
 affinity:
   nodeAffinity:
     requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values: 
            - Large
            - Medium
            
            
Available

    requiredDuringSchedulingIgnoredDuringExecution
    preferredDuringSchedulingIgnoredDuringExecution

Planned

    requiredDuringSchedulingRequiredDuringExecution
    preferredDuringSchedulingRequiredDuringExecution
    
  =====================================================================================================
  Resource Limits:
  ================
  By default, k8s sets resource limits to 1 CPU and 512Mi of memory
Each node has a set of CPU, Memory and Disk resources available.

If there is no sufficient resources available on any of the nodes, kubernetes holds the scheduling the pod.
You will see the pod in pending state. If you look at the events, you will see the reason as insufficient CPU.

Resource Requirements
==================================
By default, K8s assume that a pod or container within a pod requires 0.5 CPU and 256Mi of memory. This is known as the Resource Request for a container.

. A pod in k8 will run with no limits on cpu and memory.
. You can optionally specify how much cpu and memory each container needs

. CPU is specified in units of cores and memory is specified in units of bye

. Requests define the minimum  amount of resources that containers need.

.  Limits define the max amount of resources that the container can consume.


kubectl run nginx --image=nginx --restart=Never --requests='cpu=100m,memory=256Mi' --limits='cpu=200m,memory=512Mi'

kubectl create quota myrq --hard=cpu=1,memory=1G,pods=2 --dry-run=client -o yaml


============================================================================================================

DaemonSets
===========
DaemonSets are like replicasets, as it helps in to deploy multiple instances of pod. But it runs one copy of your pod on each node in your cluster.
Creating a DaemonSet is similar to the ReplicaSet creation process.
For DaemonSets, we start with apiVersion, kind as DaemonSets instead of ReplicaSet, metadata and spec

DaemonSets - UseCases -> mointoring and loging,proxy,weavenet

======================================================================

Static Pod:

How do you provide a pod definition file to the kubelet without a kube-apiserver?

The designated directory can be any directory on the host and the location of that directory is passed in to the kubelet as an option
while running the service.
created by kubelet
deploy controlplane components as static pods


deamonsets
deployed by kube-apiservers
deploy monitoring agents and nodes.
static and daomonsets are ignore by scheduler

The option is named as --pod-manifest-path. in kubelete.service

antherway

Instead of specifying the option directly in the kubelet.service file, you could provide a path to another config file using the config option, 
and define the directory path as staticPodPath in the file.

staticPodPath = /etc/kubernetes/manifets
docker ps
kubectl get pods -n kube-system


==============================

Multiple Schedulers

Your kubernetes cluster can schedule multiple schedulers at the same time.
$ wget https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-scheduler

https://github.com/kodekloudhub/certified-kubernetes-administrator-course/blob/master/docs/03-Scheduling/18-Multiple-Schedulers.md

  schedulerName: my-custom-scheduler
==============================================================

apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
 containers:
 - name: ubuntu-sleeper
   image: ubuntu-sleeper
   command: ["sleep2.0"] == TO  entrypoint dockerI
   args: ["10"]         == TO CMD in docker


=================================================================
 configMap
  ==========
  
  A ConfigMap is an API object used to store non-confidential data in key-value pairs. 
  Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.
  
  Use a ConfigMap for setting configuration data separately from application code.
  
  
  
  There are 2 phases involved in configuring ConfigMaps.

    First, create the configMaps
    Second, Inject then into the pod.

There are 2 ways of creating a configmap.

    The Imperative way
    
 $kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod
$ kubectl create configmap app-config --from-file=app_config.properties (Another way)


  
  kubectl create cm morevar --from-literal=mysql=passwodofme --from-literal=var3=vzg --dry-run=client -o yaml

  kubectl create cm mycm --from-file=variables --dry-run=client -o yaml > a.yml
  
    containers:
  - image: nginx
    name: pod1
    envFrom:
      - configMapRef:
          name: mycm
          
          
          =============
          
         examples : 
   Using ConfigMaps as files from a Pod :
   
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    configMap:
      name: myconfigmap

  ================================================================================================
  
 Secerts:
 
 A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. 
 Such information might otherwise be put in a Pod specification or in a container image. 
Using a Secret means that you don't need to include confidential data in your application code.

Because Secrets can be created independently of the Pods that use them, there is less risk of the Secret (and its data) being exposed during 
the workflow of creating, viewing, and editing Pods. 
 
 
 kubectl create secret generic mysecret --from-file=ssh-privatekey=/root/.ssh/id_rsa --from-literal=passphrase=password
 
 kubectl create secret generic mysecret --from-literal=username=ravikiran --from-literal=passphrase=password --dry-run=client -o yaml > sec.yaml   

  echo  'cmF2aQ==' | base64 -d
  echo -n 'ravi' | base64
  
  apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: mysecret
      optional: false # default setting; "mysecret" must exist
      
      ========
      
      apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
  volumes:
  - name: foo
    secret:
      secretName: mysecret
      defaultMode: 0400
  
  
   - image: nginx
    name: pod2
    envFrom:
      - secretRef:
          name: mysecret
          
          =================
          
  apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: mycontainer
    image: redis
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: username
            optional: false # same as default; "mysecret" must exist
                            # and include a key named "username"
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: password
            optional: false # same as default; "mysecret" must exist
                            # and include a key named "password"
  restartPolicy: Never
   
   =============================
  envFrom:
      - secretRef:
          name: mysecret
  
  envFrom:
      - configMapRef:
          name: filename
          
      ============================
      
Config maps



kubectl create cm mycm --from-file=a.txt
kubectl get cm
kubectl describe cm mycm

kubectl create cm mycm --from-file=a.txt --dry-run=client -o yaml

We see this using by creating it using test pod

kubectl create cm variables --from-file=variables

 kubectl run busybox --image=busybox  --env=team=noc  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml

kubectl exec -it busybox -- env

kubectl get cm -o yaml

    
kubectl create secret generic sec --from-file=ssh-private=/root/.ssh/known_hosts --from-literal=passphrase=ravikiran

 kubectl create secret generic db-user-pass  --from-file=a.txt  --from-file=pass.txt --dry-run=client -o yaml

kubectl create secret generic sec --from-file=usercred=ravikiran --from-literal=passphrase=ravikiran300 --dry-run=client -o yaml











==================

Multicontainers

// create the pod

kubectl create -f simple-pod.yml// access logs

kubectl logs busybox -c busybox1
kubectl logs busybox -c busybox2
kubectl logs busybox -c busybox3// exec into running containers


Mutlicontainer in POD
========================

kubectl run busybox --image=busybox  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml

kubectl exec -it busybox -- /bin/bas

images u can use for minimal container
=========================================

alpine
busybox
centos:latest 
ubuntu:20.04

kubectl exec -it busybox -c busybox1 /bin/sh
kubectl exec -it busybox -c busybox2 /bin/sh
kubectl exec -it busybox -c busybox3 /bin/sh



 kubectl run busybox --image=busybox  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml


kubectl exec -it busybox -- /bin/bash


================================================
Environment:

Of course, we have solved this problem. You can use environment variables and configuration files to store this information in a “central location” and reference 
these from our app. When you want to change the configuration, simply change it in the file or change the environment variable, 
and you are good to go! No need to hunt down every location where the data is referenced.



 kubectl run busybox --image=busybox  --env=team=noc  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml

 kubectl exec busybox -- printenv

 kubectl exec -it busybox -- env

 kubectl exec -it busybox -- /bin/sh


==========================================

Config maps

kubectl create cm mycm --from-file=a.txt
kubectl get cm
kubectl describe cm mycm

kubectl create cm mycm --from-file=a.txt --dry-run=client -o yaml

We see this using by creating it using test pod

kubectl create cm variables --from-file=variables

 kubectl run busybox --image=busybox  --env=team=noc  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml

kubectl exec -it busybox -- env

kubectl get cm -o yaml

    
kubectl create secret generic sec --from-file=ssh-private=/root/.ssh/known_hosts --from-literal=passphrase=ravikiran

 kubectl create secret generic db-user-pass  --from-file=a.txt  --from-file=pass.txt --dry-run=client -o yaml

kubectl create secret generic sec --from-file=usercred=ravikiran --from-literal=passphrase=ravikiran300 --dry-run=client -o yaml

=====================================================================================================================================


Node node01 Unschedulable
Pods evicted from node01 

#We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable
kubectl drain node01 --ignore-daemonsets

Node01 is Schedulable
kubectl uncordon node01

Run: kubectl get pods -o wide and you will see that there is a single pod scheduled on node01 which is not part of a replicaset.
The drain command will not work in this case. To forcefully drain the node we now have to use the --force flag.

We need to carry out a maintenance activity on node01 again. Try draining the node again using the same command as before: 

A forceful drain of the node will delete any pod that is not part of a replicaset

This means that both nodes have the ability to schedule workloads on them
 kubectl describe nodes controlplane | grep -i taints 
 
 To get latest version of k8 cluster
 kubectl upgrade plan
 
 We will be upgrading the master node first. Drain the master node of workloads and mark it UnSchedulable
 There are daemonsets created in this cluster, especially in the kube-system namespace. To ignore these objects and drain the node, we can make use of the --ignore-daemonsets flag
 kubectl drain controlplane --ignore-daemonsets
 
 
 In order to ensure minimum downtime, upgrade the cluster one node at a time, while moving the workloads to another node. In the upcoming tasks you will get to practice how to do that
 
 Upgrade the controlplane components to exact version v1.20.0

Upgrade kubeadm tool (if not already), then the master components, and finally the kubelet. Practice referring to the kubernetes documentation page. Note: While upgrading kubelet, if you hit dependency issue while running the apt-get upgrade kubelet command, use the apt install kubelet=1.20.0-00 command instead
 
 On the controlplane node, run the command run the following commands:

    apt update
    This will update the package lists from the software repository.

    apt install kubeadm=1.20.0-00
    This will install the kubeadm version 1.20

    kubeadm upgrade apply v1.20.0
    This will upgrade kubernetes controlplane. Note that this can take a few minutes.

    apt install kubelet=1.20.0-00 This will update the kubelet with the version 1.20.

    You may need to restart kubelet after it has been upgraded.
    Run: systemctl restart kubelet

============

Upgrade the controlplane components to exact version v1.20.0

Upgrade kubeadm tool (if not already), then the master components, and finally the kubelet. Practice referring to the kubernetes documentation page. Note: While upgrading kubelet, if you hit dependency issue while running the apt-get upgrade kubelet command, use the apt install kubelet=1.20.0-00 command instead

make it schudable again
kubectl uncordon controlplane

====================================
kubectl get po -n kube-system

Look at the ETCD Logs using the command kubectl logs etcd-controlplane -n kube-system or check the image used by the ETCD pod: 

kubectl describe pod etcd-controlplane -n kube-system

At what address can you reach the ETCD cluster from the controlplane node?

Check the ETCD Service configuration in the ETCD POD
kubectl -n kube-system describe pod etcd-controlplane | grep '\--listen-client-urls'

etc-cert file
kubectl -n kube-system describe pod etcd-controlplane | grep '\--cert-file'

root@controlplane:~# kubectl -n kube-system describe pod etcd-controlplane | grep '\--trusted-ca-file'

snapshot
===========

The master nodes in our cluster are planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.

Store the backup file at location /opt/snapshot-pre-boot.db

se the etcdctl snapshot save command. You will have to make use of additional flags to connect to the ETCD server.
--endpoints: Optional Flag, points to the address where ETCD is running (127.0.0.1:2379)
--cacert: Mandatory Flag (Absolute Path to the CA certificate file)
--cert: Mandatory Flag (Absolute Path to the Server certificate file)
--key:Mandatory Flag (Absolute Path to the Key file) 

ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db


Snapshot saved at /opt/snapshot-pre-boot.db


Restore the snapshot
====================

Restore the etcd to a new directory from the snapshot by using the etcdctl snapshot restore command. Once the directory is restored, update the ETCD configuration to use the restored directory.

root@controlplane:~# ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db


2021-03-25 23:52:59.608547 I | mvcc: restore compact to 6466
2021-03-25 23:52:59.621400 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32

Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the --data-dir.

Next, update the /etc/kubernetes/manifests/etcd.yaml:

We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory (/var/lib/etcd) to the new directory /var/lib/etcd-from-backup.

 volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
    
    With this change, /var/lib/etcd on the container points to /var/lib/etcd-from-backup on the controlplane (which is what we want)

When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the /etc/kubernetes/manifests directory.

    Note: as the ETCD pod has changed it will automatically restart, and also kube-controller-manager and kube-scheduler. Wait 1-2 to mins for this pods to restart. You can run a watch "docker ps | grep etcd" command to see when the ETCD pod is restarted.

    Note2: If the etcd pod is not getting Ready 1/1, then restart it by kubectl delete pod -n kube-system etcd-controlplane and wait 1 minute.

    Note3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.

If you do change --data-dir to /var/lib/etcd-from-backup in the YAML file, make sure that the volumeMounts for etcd-data is updated as well, with the mountPath pointing to /var/lib/etcd-from-backup (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)

===========================

Taints and Tolerations
=======================

Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule

    Key = spray
    Value = mortein 

kubectl taint nodes node01 spray=mortein:NoSchedule

kubectl describe node01 | grep -i taints

=======

Create another pod named bee with the NGINX image, which has a toleration set to the taint mortein.

    Image name: nginx
    Key: spray
    Value: mortein
    Effect: NoSchedule
    Status: Running
    
    ===
    
    Create another pod named bee with the NGINX image, which has a toleration set to the taint mortein.

    Image name: nginx
    Key: spray 
    
    ==============================
  
  Node Affinity
  ==============
  
Set Node Affinity to the deployment to place the pods on node01 only.

    Name: blue
    Replicas: 3
    Image: nginx
    NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
    Key: color
    values: blue 
    
    apiVersion: v1
kind: Pod
metadata:
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    value: mortein
    effect: NoSchedule
    operator: Equal
    
    remove taint:
    check where is taint in mastet or node
    
    kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-
    
    
    ========================
    
     kubectl label node node01 color=blue
     
     Check if controlplane and node01 have any taints on them that will prevent the pods to be scheduled on them. If there are no taints, the pods can be scheduled on either node.
So run the following command to check the taints on both nodes.
kubectl describe node controlplane | grep -i taints
kubectl describe node node01 | grep -i taints





Update the deployment by running kubectl edit deployment blue and add the nodeaffinity section as follows:

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue

==================================
Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.

Use the label - node-role.kubernetes.io/master - set on the controlplane node.

    Name: red
    Replicas: 2
    Image: nginx
    NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
    Key: node-role.kubernetes.io/master
    Use the right operator 



apiVersion: apps/v1
kind: Deployment
metadata:
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: Exists
                
                
                
 =========================================================================================================================================
 ==========================================================================================================================================
 
 Volumes
 ======
 volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}
    
    
 image: busybox
    name: busybox
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: DirectoryOrCreate
      
      
     =============================================
 
#https://betterprogramming.pub/tutorial-how-to-use-kubernetes-job-and-cronjob-1ef4ffbc8e84

look into this article about jobs and cron jobs
     
     Job:
     kubectl create job1 rav --image=busybox -o yaml  --dry-run=client   -- /bin/sh -c 'date; echo sleeping....; sleep 90s; echo exiting...'


Enforcing a time limit
For e.g., you are running a batch job and it takes too long to finish due to some reason. This might be undesirable. 
You can limit the time for which a Job can continue to run by setting the activeDeadlineSeconds attribute in the spec.

spec:
  activeDeadlineSeconds: 5
  template:

=====

Handling failures

What if there are issues due to container failure (process exited) or Pod failure? Let's try this out by simulating a failure.

In this Job, the container prints the date, sleeps for 5 seconds, and exits with a status 1 to simulate failure.

here set sleep 5

spec:
  backoffLimit: 2
  template:

==================

cronjob
========

kubectl create cronjob busybox --image=busybox --schedule="*/1 * * * *"   -o yaml --dry-run=client -- /bin/sh -c 'date; echo Hello from the Kubernetes cluster'


serives
======

.  A Service enables network access to a set of Pods in Kubernetes.
.  Kubernetes gives Pods their own IP addresses   and can load-balance across them


. ClusterIP is the default and most common service type.
 Kubernetes will assign a cluster-internal IP address to ClusterIP service.
  This  makes the  service only reachable within the cluster.
 You cannot make requests to service (pods) from outside the cluster. 
 Inter service communication within the cluster


=========================

NodePort:

. This service visible outside the Kubernetes cluster by the node’s IP address and the port number declared . 
  The service also has  to be of type NodePort (if this field isn’t specified, Kubernetes will allocate a node port automatically). 
. Node port must be in the range of 30000–32767. Manually allocating a port to the service is optional

========================================

LoadBalancer:

 Every time you want to expose a service to the outside world, you have  to create a new        LoadBalancer  and get an IP address.
 It exposes the service externally using a cloud providers load Balancer
 Each cloud provider (AWS, Azure, GCP,) has its own native load balancer implementation. The cloud provider will create a load balancer, which then automatically routes requests to your Kubernetes Service.



Readiness & Liveness in PODS:

. This is used for testing. Before the POD available  readinees  probe will check whether your defined service is ready or not . if running Pod gets available.

. Liveneess is like after pod gets started and check the service defined is running or not. like it continous probe the service

. The kubelet uses readiness probes to know when a container is ready to start 

. The kubelet  uses liveness probes to know when to restart a container.



#https://github.com/kodekloudhub/certified-kubernetes-administrator-course/blob/master/docs/02-Core-Concepts/19-Services.md
LoadBalacer
#https://github.com/dennyzhang/cheatsheet-kubernetes-A4

kubectl expose deployment/my-demo-website --type=LoadBalancer --dry-run=client -o yaml > svc.yaml
kubectl expose deployment/mydemo --type=LoadBalancer --dry-run=client -o yaml > svc.yaml

NodePort:
========
 kubectl expose deployment/mydemo --type=NodePort --dry-run=client -o yaml  >

ClusterIP:
========

https://matthewpalmer.net/kubernetes-app-developer/articles/service-kubernetes-example-tutorial.html

Cluster IP is a virtual IP that is allocated by the K8s to a service. It is K8s internal IP.

A Cluster IP makes it accessible from any of the Kubernetes cluster’s nodes. The use of virtual IP addresses for this purpose makes it possible to have several pods expose the same port on the same node – All of these pods will be accessible via a unique IP address.

This IP is stable and never changes in the service lifecycle(unless deleted explicitly).

2 different pods can communicate using this IP, though I recommend using cluster DNS service

kubectl expose deployment/mydemo --dry-run=client -o yaml

=========

autoscale
=========

kubectl autoscale deploy/mydemo --min=5 --max=10 --cpu-percent=80 --dry-run=client -o yaml

kubectl create quota myrq --hard=cpu=1,memory=1G,pods=2 --dry-run=client -o yaml
===========================================================================================

A service account provides an identity for processes that run in a Pod.

Processes in containers inside pods can also contact the apiserver. When they do, they are authenticated as a particular Service Account (for example, default)


stateful set and headless service in k8s

======================================================================================================================

. An emptyDir volume is first created when a Pod is assigned to a Node and exists as long  as that Pod is running on that node.
. As the name says, it is initially empty. All Containers in the same Pod can read and write in the same emptyDir volume.
. When a Pod is restarted or removed, the data in the emptyDir is lost forever.

. A hostPath volume mounts a file or directory from the node's filesystem into the Pod. You can specify whether the file/directory must already exist on the node or should be created on pod startup. You can do it using a type attribute in the config file:


Persistent Volume (PV) is an abstraction for the physical storage device that you have attached to the cluster. Pods can use this storage space using Persistent Volume Claims (PVC).
PV is independent of the lifecycle of the Pods. It means that data represented by a PV continue to exist as the cluster changes and as Pods are deleted and recreated.
 PV is not Namespaced, it is available to whole cluster. i.e PV is accessible to all namespaces.

 Kubernetes looks for a PV that meets the criteria defined in the PVC, and if there is one, it matches claim to PV.
 PVC must be in same namespace as the Pod. For each Pod, a PVC makes a storage consumption request within a namespace.


apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
    
  
. StorageClass provisions PV dynamically, when PVC claims it.

. StorageClass allows dynamically provision volumes for an incoming claim.

. StorageClass is used in conjunction with PVC that allow Pods to dynamically request a new storage.

. StorageClass use provisioners that are specific to the storage platform or cloud provider to give Kubernetes access to the physical storage.




Volumes
 ======
 volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}
    
    
 image: busybox
    name: busybox
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: DirectoryOrCreate
      
      
     =============================================
     
     Readiness & Liveness in PODS:
     
     
 . This is used for testing. Before the POD available  readinees  probe will check whether your defined service is ready or not . if running Pod gets available.

. Liveneess is like after pod gets started and check the service defined is running or not. like it continous probe the service

. The kubelet uses readiness probes to know when a container is ready to start 

. The kubelet  uses liveness probes to know when to restart a container.


apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5


==================

 . This is used for testing. Before the POD available  readinees  probe will check whether your defined service is ready or not . if running Pod gets available.


apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: k8s.gcr.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20




=======================================

In Kubernetes, an init container is the one that starts and executes before other containers in the same Pod.
It’s meant to perform initialization logic for the main application hosted on the Pod.
For example, create the necessary user accounts, perform database migrations, create database schemas and so on

Init containers are exactly like regular containers, except:

Init containers always run to completion.
Each init container must complete successfully before the next one starts.

Init containers support all the fields and features of app containers, including resource limits, volumes, and security settings. However, the resource requests and limits for an init container are handled differently, as documented in Resources.

Also, init containers do not support lifecycle, livenessProbe, readinessProbe, or startupProbe because they must run to completion before the Pod can be ready

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]


    





 
 
 
